---
title: "Unlocking dusty data with OCR"
subtitle: "A (UCB student's) library guide"
author: "Ross Mattheis"
output: html_notebook
---

```{r, include= FALSE}
require(RCurl)
require(dplyr)
require(stringr)
require(ggplot2)
require(stringdist)

```

If you were an administrator (let's say, a university registrar or the manager of a local unemployment relief office) the way you kept record of the folks you serviced (students, unemployed workers) would be entierly dependent on the time you were alive. Before the twentieth century, you probably made records by hand, filling out printed forms or writing in legders. For much of the twentieth century, you would have made use of a typewriter (of some sort), and, More recently, you'd enter and store records digitally. The differences in these three major periods of record keeping matter for folks who would like to recover information trapped in these records. For the most recent period, data is almost immediately accessible: researchers may only have to stumble through an outdated file format to use administrative records. In the earliest period, there is (to date) no effective technology more efficient than manual transcription.      

<center>
![History from the perspective of a data-monger](https://raw.githubusercontent.com/wrathofquan/data-science-fellowship/master/Ross/code/Sources_ocr.png)
</center>

In this brief notebook, I'll illustrate the process of applying OCR to directories of the board of directors of New York banks in the years 1899, 1901, and 1903. 

Directories are particularly well suited documents for OCR. The general strategy taken here is to abandon almost all structure in the original image, and then use patterns in the raw text to re-introduce formatting. Examples of useful patterns could be indentations, alternation of numeric and textual information, meaningful commas or other punctuation, and so on. However, lines demarcating columns of a table tend not to be transcribed. So this approach will typically not work well for tables of numbers and will work well for directories, catalogues, and other _partially_ formatted records. 

Here's a brief outline of the OCR workflow:

### Four step approach

- Step one: Scan the document.
- Step two: Apply OCR software.
- Step three: Use patterns in the records to format raw text data. 
- Step four: Take advantage of multiple scans to limit errors. 

# Scan

Before jumping into the more technical steps of the process, a word on scanning. A substantial portion of typed sources have already been digitised as part of a collaborative project between academic libraries and Google, [hathitrust](https://www.hathitrust.org/). More likely than not, one or more scanned copies of the source you would like to use have already exist. If your source hasn't yet been scanned, reach out to a librarian before you start scanning hundreds of pages. Librarians can help disseminate the final scanned document, and may even be willing to help in the digitization process.  

The scans used in this exercise are from Hathitrust. 

# Applying OCR

Fortunately, the problem of recovering text from a scanned document seems essentially solved. 

There are many OCR tools including freeware (Tesseract, Gocr, Ocrad, ...) and proprietary software (Adobe, ABBY, ...).


```{r, include = FALSE}
# Loading raw text data
txt_adobe_1899 <- read.csv("https://raw.githubusercontent.com/wrathofquan/data-science-fellowship/master/Ross/data/txt_hathi_1899.csv")
txt_adobe_1901 <- read.csv("https://raw.githubusercontent.com/wrathofquan/data-science-fellowship/master/Ross/data/txt_adobe_1901.csv")
txt_adobe_1903 <- read.csv("https://raw.githubusercontent.com/wrathofquan/data-science-fellowship/master/Ross/data/txt_hathi_1903.csv")

txt_adobe <- rbind(txt_adobe_1899, txt_adobe_1901, txt_adobe_1903)

txt_hathi_1899 <- read.csv("https://raw.githubusercontent.com/wrathofquan/data-science-fellowship/master/Ross/data/txt_hathi_1899.csv")
txt_hathi_1901 <- read.csv("https://raw.githubusercontent.com/wrathofquan/data-science-fellowship/master/Ross/data/txt_hathi_1901.csv")
txt_hathi_1903 <- read.csv("https://raw.githubusercontent.com/wrathofquan/data-science-fellowship/master/Ross/data/txt_hathi_1903.csv")

txt_hathi <- rbind(txt_hathi_1899, txt_hathi_1901, txt_hathi_1903)

rm(txt_adobe_1899)
rm(txt_adobe_1901)
rm(txt_adobe_1903)
rm(txt_hathi_1899)
rm(txt_hathi_1901)
rm(txt_hathi_1903)

txt_adobe$text <- as.character(txt_adobe$text)
txt_hathi$text <- as.character(txt_hathi$text)

```

Here, I'll use the original OCR transcription from Hathitrust (via Tesseract), and Adobe. Here's an example of the text output from each:

```{r}
head(txt_adobe$text)
```

```{r}
head(txt_hathi$text)
```

### From OCR to raw text 

While applying OCR, there are a few steps that will help greatly in the cleaning process later on. 

Before each new use of OCR, clear the PDF of `hidden data' (i.e. text).

Crop the document to remove any unecessary information, and delete unused pages. 

Apply your favorite OCR tool to the document. 

Split the document into individual pages, keeping track of the page number. Later on, we will want to compare individual lines of transcribed text

Extract the text from the document. In Linux, this can be done from the command line with 'pdftottext'. Here's an example of code to extract text from all .pdf files in a directory. 

```{c}

for file in *.pdf
  do pdftotext -layout "$file" "$file.txt"
  done

```

The next step is to convert the set of raw text files into a .csv in which each line of the text file is a row of the table. In R, this can be done with `read.table()` by separating rows by the symbol denoting the end of the line, which may be `\n` or `$` depending on the encoding. Alternatively, there is a built-in function in R, `readLines()` that will do this automatically.   


# Recover formating from raw text





# Using multiplicity







